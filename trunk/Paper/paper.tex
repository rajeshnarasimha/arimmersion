\documentclass{acmsiggraph}                     % final
%\documentclass[annualconference]{acmsiggraph}  % final (annual conference)
%\documentclass[review]{acmsiggraph}            % review
%\documentclass[widereview]{acmsiggraph}        % wide-spaced review
%\documentclass[preprint]{acmsiggraph}          % preprint

%% Uncomment one of the five lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and ``final'' is for
%% the version to be printed. The ``final'' variant will accept the 
%% ``annualconference'' parameter, which changes the height of the space
%% left clear for the ACM copyright information.

%% The 'helvet' and 'times' packages define the typefaces used for
%% serif and sans serif type in this document. Computer Modern Roman 
%% is used for mathematics typesetting. The scale factor is set to .92
%% to bring the sans-serif type in line with the serif type.

\usepackage[scaled=.92]{helvet}
\usepackage{times}

%% The 'graphicx' package allows for the inclusion of EPS figures.

\usepackage{graphicx}

%% use this for zero \parindent and non-zero \parskip, intelligently.

\usepackage{parskip}

%% Optional: the 'caption' package provides a nicer-looking replacement
%% for the standard caption environment. With 'labelfont=bf,'textfont=it',
%% caption labels are bold and caption text is italic.

\usepackage[labelfont=bf,textfont=it]{caption}

%% If you are submitting a paper to the annual conference, please replace 
%% the value ``0'' below with the numeric value of your OnlineID. 
%% If you are not submitting this paper to the annual conference, 
%% you may safely leave it at ``0'' -- it will not be included in the output.

\onlineid{0}

%% Paper title.

\title{A Study of an X-Ray Vision Tracking Task in Simulated Augmented Reality}

%% Author and Affiliation (single author).

%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\Allied Widgets Research}

%% Author and Affiliation (multiple authors).

\author{
Submission 191
%Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ Starbucks Research %
%\and Ed Grimley\thanks{e-mail:ed.grimley@aol.com}\\Nigel Mansell\thanks{nigelf1@msn.com}\\ Grimley Widgets, Inc. %
%\and Martha Stewart\thanks{e-mail:martha.stewart@marthastewart.com}\\ Martha Stewart Enterprises \\ Microsoft Research
}

%% Keywords that describe your work.

\keywords{
%radiosity, global illumination, constant time
}

%%%%%% START OF THE PAPER %%%%%%

\begin{document}

%\teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
%}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

\maketitle

%% Abstract section.

\begin{abstract}

This is an AR study, simulated using a virtual reality (VR), to examine the effects of level of immersion and tracking sensor reliability in AR.  We analyze participant performance on a tracking task in which the AR interface provides x-ray vision to help track a moving target.  We varied the field of view of the AR display, as well as the reliability of the head tracker.  In low reliability conditions, we simulate tracker failure by disabling the augmented view of the scene for brief periods.  Our study gives insight into the effect of tracking sensor reliability on user performance in a tracking task, as well as the relationship between reliability and field of view in an AR system.

\end{abstract}

%% ACM Computing Review (CR) categories. 
%% See <http://www.acm.org/class/1998/> for details.
%% The ``\CRcat'' command takes four arguments.

\begin{CRcatlist}
%  \CRcat{K.6.1}{Management of Computing and Information Systems}{Project and People Management}{Life Cycle};
%  \CRcat{K.7.m}{The Computing Profession}{Miscellaneous}{Ethics}
\end{CRcatlist}

%% The ``\keywordlist'' command prints out the keywords.
\keywordlist

\section{Introduction}

%% The ``\copyrightspace'' command must be the first command after the 
%% start of the first section of the body of your paper. It ensures the
%% copyright space is left at the bottom of the first column on the first
%% page of your paper.

%% \copyrightspace

Augmented Reality systems may be used to simulate x-ray vision \cite{1383060}.  The ability to see through solid objects has potential applications in various fields ranging from construction \cite{Webster96augmentedreality}, medicine \cite{azuma95survey}, military \cite{Livingston02anaugmented} and search-and-rescue operations \cite{1528424}.  Unfortunately, implementing x-ray vision is not a trivial task.  AR systems have cost issules, including those pertaining to level-of-immersion.  For example, what is the ideal augmented field of view (FOV) for a given scenario?  Other issues relate to tracking the objects in the x-ray view, such as GPS dropout errors \cite{4079263}, where tracking is temporarily lost.

In this study, we have implemented an AR simulation with virtual reality hardware and software to examine the effects of varying FOV and sensor dropouts in a tracking task.  Using an AR simulation gives us the ability to vary experiment conditions which may be difficult or even impossible to replicate on a real-world AR system.  It also gives us the ability to remove unnecessary factors from generating noise in our data, such as illumination or visibilty changes, weather problems, or environmental distractions. 

The AR simulation is built as a tracking task, where the participant is placed inside a virtual room and visually follows a person outside, walking an unpredictable path.  We wish to answer the following questions.  What effect does varying the augmented FOV have on user tracking performance?  What effect do different dropout lengths have on performance?  What are the interactions between these two variables?

\section{Related Work}

Livingston and Ai studied the effects of various sources of registration error with an AR simulation similar to our own \cite{4637329}.  Here, participants tracked a virtual car moving throughout a real environment, with a white box representing an augmented view of the car's location.  A white box was continuously visible, even when the car itself was occluded by a building in the environment.  Other virtual cars and their associated white boxes acted as distractors.  At specific times during the experiment, the simulation would freeze and the participant was asked to align the center of their view (indicated by cross-hairs) on the location of the correct car and press a key.  While the effects of several types of error were studied in this work, the effect of registration dropouts was not examined.

Bane and Hollerer propose a set of AR tools for virtual x-ray vision \cite{1383060}.  Various augmented views are presented, each attempting to overcome the "Superman's X-ray vision" problem of presenting too much AR information to the user.  The toolset's usefulness is illustrated with an example of an outdoors user viewing the contents of a nearby building.

% TODO: add discussion of validity of AR simulation w/ citations

%other studies which vary AR fov, tracking failures?

\section{AR Simulation}

Our experiment involves a tracking task which uses an augmented reality interface to allow people to be tracked behind walls.  A participant stands in the middle of a square room with doors and windows on all sides.  Virtual people walk unpredictable paths outside the building.  The target to be tracked is one of these people, visually distinguishable by a large black top hat.  The augmented reality interface overlays a translucent red rectangle on each person, which is visible even when the person is occluded by a wall.  However, the target person's overlay is identical to the other virtual people overlays; the target may only be distinguished when visible through a window or a door.

To actually run this experiment using a true augmented reality system would be extremely difficult.  Many confederates would be needed to walk around the room, and their movements would need to be accurately reproduced for each new participant.  Also, the confederates would need to be continously tracked for the augmented overlays to be displayed.  Instead, we chose to simulate the experiment setup in a completely virtual environment.

By simulating augmented reality, we are freed from the current restrictions of tracking and display technology.  This simulated environment also affords easily controlled variables, and thus the experiment may be replicated.

\section{Experimental Design}

The design of the experiment is built with several goals in mind.  The task should have straightforward, quantitative results.  Also, the task should be reasonably simple so as not to frustrate the participants.  Further, the task should be generic so as to be generizable, yet grounded in reality (i.e. not an abstract world), to enhance participant familiarity with the AR task.

We developed a tracking task scenario, where the user is asked to visually track a virtual person as it moves throughout the scene.  The participant is centered inside a room with various doors and windows giving a view to the outside world.  They may change their orientation as the please, but not change position (3 degrees of freedom).  The virtual person to be tracked walks an unpredictable path outside the room.  The user's view to the virtual person may be occluded at any time by the walls of the building; at other times the virtual person may be visible through doors or windows in the building.  The augmented view element is composed of red marker rectangles indicating the location of the tracked person and other virtual people.

\begin{figure}[t]
	\centering
	\includegraphics[width=3in]{figures/augmentedroom.png}
	\caption{Overhead view of virtual room (approx. measure).  Participant is stationary in the center of the room, with initial orientation along ``user view'' arrow.  Top-hat's initial position is outside the front doorway, directly in the participant's view.}
\end{figure}

Participants are presented a view of the scene through a head-mounted display helmet with orientation tracking.  The tracking overlays are augmented on top of the original image to form the simulated augmented reality view.

The parameters of our experiment are divided between the ``real'' immersion factors, such as the field of view of the HMD, and the ``augmented'' immersion factors, such as the field of view of the simulated AR display, and the performance of the tracking sensor.  We introduce periods of sensor dropouts, where the augmented overlays disappear, to simulate the effect of failures in a real tracking system.  For example, such dropouts might occur with a magnetic tracker near interfering materials, or with visual tracking when tracked features are lost.

In our experiment we vary two parameters: the field of view of the AR interface and the length of tracking failure periods.  Each trial is 60 seconds, with seven tracking failures.  The total vertical field of view of the HMD is 36 degrees; the three possible values for the augmented field of view were 10, 20, and 34 degrees.  The length of dropouts varied between two seconds (highest), one second (medium) and zero seconds (lowest).  We experimented with longer dropout periods but deemed them too long.  This gave us a total of 9 conditions, and for each participant we tested each condition 3 times giving a total of 27 trials per participant.

During each trial there are 20 people in addition to a virtual man \emph{Top Hat} wearing a tall black top hat walking an unpredictable path outside of the room.  The paths for each virtual person was randomly generated from set of coordinates exterior to the room.  Each path had the constraint that they must stay within the rectangular $50\times50$ $m^2$ area surrounding the room.  At each path point, the person will randomly change his/her walking speed within the range of 4 to 7 m/s.  Top Hat had the additional constraints as follows.  He must start at the same location for each trial which made him visible by the participant through the front door the building.  In order to make the paths by of similar difficulty Top Hat must walk around the entire building at least once, and he must be visible through the windows for at least 10 seconds.  These constraints were met by randomly generating paths until all were satisfied.  We generated 27 sets of paths in this fashion, each corresponding to a specific trial.  We used a latin squares ordering to discourage the effects of learning.

\subsection{Implementation}

We implemented this experiment with Python and the Vizard virtual reality toolkit.  Head-tracking was handled by a InterSense InertiaCube3 orientation tracker with a 180 Hz refresh rate.  We used a Pro-View 60 head mounted display (HMD) displaying $640\times480$ video at 60 Hz, and 36 degrees vertical field of view.  As we were not concerned the effects of stereo, we turned off the stereoscopic feature.  The experiment ran on a 2.4 GHz Intel Core 2 machine with 2 Gb of RAM and a NVIDIA GeForce 9800 GX2 graphics card running Windows XP SP3.

\subsection{Participants}

We ran this experiment on 19 subjects, 11 were male, and 8 female.  The participants ranged in age from 23 to 59, 15 were in the range 23 to 30, and the remaining 4 in the range 51 to 49.  None were colorblind.  Most users were heavy computer users with at least somewhat familiarity with virtual reality and augmented reality.  Only one participant played 3D video games often.  After every 9 trials we had the participants take a mandatory 5 minute break.  Some experienced light fatigue, but none experienced any strong effects of motion sickness.

\begin{figure}[t]
	\centering
	\includegraphics[width=3in]{figures/tophatscreenshot.png}
	\caption{Sample user-view of AR simulation.  \emph{Top Hat} is visible (non-occluded) in the doorway.  The augmented view overlays transparent red rectangles on the characters, indicating their location while occluded.}
\end{figure}

\section{Analysis}

In our experiment we define performance as effectiveness in tracking Top Hat.  We measure performance by recording the angular distance (in yaw) between the the user's viewing direction and the direction towards the target character.  This measure ranges between zero and 180 degrees (we used the smaller of the two options, clockwise and counter-clockwise).  We record one measurement per video frame, at about sixty frames per second, resulting in about 3600 measurements for a one second trial.

Because the video does not always run at precisely 60 Hz, each trial might have a slightly different number of measurements.  We also record a timestamp for each measurement, so that we can determine their exact frequency.  As a pre-processing step, we linearly re-sample the data for each trial so that each has exactly 3600 measurements at 60 Hz.

We observed that participants tend to switch between two different states during the experiment: a tracking state, where they are following the target; and a lost state, where they are searching for the target.  The two states are easily visible in the data.  In the tracking state, the angular error is generally low.  We asked participants to keep the cross hairs on the target as closely as they could, but different participants achieved different levels of accuracy.  In the lost state, the error starts to steadily climb, and may fluctuate depending on where the target moves.  The error may even return to near-zero during a lost state, if the participant unwittingly crosses the target's path.

% TODO: include two plots of error here (one with losses, one without)

We hypothesize that less immersive conditions will have longer and more frequent periods where the participant loses the target.  We considered several different metrics which might be appropriate in testing this hypothesis.

The simplest is the average error over an entire trial.  This may not accurately represent tracking failures, because the error does not necessarily stay high during the lost state, and also different participants may generally keep the cross hair further from the target even in the tracking state.

% median ??

We can more explicitly detect the participant's state by applying a threshold $\tau$ to the data.  The threshold specifies the maximum angular error that still represents successful tracking of the target. 
%When the error rises above the threshold, we consider the participant lost.
We use a threshold of one quarter of the total horizontal field of view of the HMD, $\tau = 12$ degrees.
% TODO: how to motivate this threshold?
\begin{figure}[t]
	\centering
	\includegraphics[width=3.5in]{figures/tt_deadlen.pdf}
	\caption{\label{fig:tt_deadlen}Box plot of the amount of time the target is near the crosshairs versus the length of sensor dropouts.}
\end{figure}
Using this threshold we measure a participant's ``time to failure,'' the time until the a lost state is encountered.  However, this may not be a very descriptive measure, since a participant may reach the lost state sooner or later depending on the movement of the avatars, which varies between trials.

We also consider a measurement we call ``time tracking,'' which is the amount of time spent in the tracking state.  This metric seems to most generally represent how well a participant performed.  Trials with either more or longer lost periods will result in a lower total time tracking.

For a more specific measure of performance, we count the number of times a participant reaches the lost state in a trial.  We only record a lost state when the error stays above the threshold $\tau$ for a minimum of 0.5 seconds.
% TODO: how to motivate this threshold?

%time tracking is overview metric -- captures both number of times lost and length of lost periods

%number of times lost is piece of overview -- number of times lost.

\section{Results}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=3in]{figures/numtimes_deadlen.pdf}
%	\caption{Box plot of the number of times the participant is lost versus the length of the sensor dropouts.}
%\end{figure}

%\begin{figure}[t]
%	\centering
%	\includegraphics[width=3in]{figures/numtimes_fov.pdf}
%	\caption{Box plot of the number of times the participant becomes lost versus the field of view of the augmentations.}
%\end{figure}

% TODO: make the units seconds and change the labels on the bottom of this figure


%\begin{figure}[t]
%	\centering
%	\includegraphics[width=3in]{figures/tt_fov.pdf}
%	\caption{Box plot of time the participant is not lost versus the field of view of the augmentations.}
%\end{figure}


% TODO: should we include data from older generation??

% TODO: look at analysis from mean.

We performed two-way {ANOVA} for the time tracking metric as an overall analysis of the effects of our two factors.  Figure \ref{fig:anova} gives the results of this analysis.  Both the failure length and the field of view have a very significant effect on performance using this metric.  
% TODO: interaction?

We also used a two-way {ANOVA} to examine the number of times the participant loses the target, with results shown in Figure \ref{fig:anova}.  Both field of view and dropout length have a significant effect.  There also is an interaction between the two factors.  Figure \ref{fig:interaction} shows a plot of this interaction.  

From this plot we can see that with a small field of view, performance is equally bad with no dropouts or one second dropouts.  The small field of view seems to have a dominating effect on performance.  When the field of view is increased to the medium level, performance dramatically increases when there are no dropouts, but only gets slightly better when there are dropouts.  Finally, we do not observe much difference in performance between the medium and large field of view cases.  Using Tukey's HSD to compare means, the medium and large field of view conditions are in fact not significantly different (p=??).

\begin{figure}[t]
	\centering
	\includegraphics[width=3.5in]{figures/numtimes_interaction.pdf}
	\caption{\label{fig:interaction}Interaction plot of the effect of field of view and sensor dropout length on the number of times the participant loses the target.}
\end{figure}

\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
%\multicolumn{2}{|c|}{Selection Stroke Thresholds} \\
Metric & Factor & F value & Pr \\
\hline
Time Tracking & Failure Length & 28.904 & $<0.001$\\
Time Tracking & Field of View & 15.800 & $<0.001$\\
Number of Times Lost & Failure Length & 8.5008 & $<0.001$\\
Number of Times Lost & Field of View & 13.060 & $<0.001$\\
\hline
\end{tabular}
 \caption{\label{fig:anova}Factors which have a significant effect.}
\end{figure}

Figure \ref{fig:tt_deadlen} shows a box plot of time tracking versus dropout length.  Performance decreases between zero and one seconds, but seems to level off between one and two.  We ran a post-hoc analysis to determine which conditions were significantly different.  Using Tukey's HSD, we found that conditions one and two were significantly different than zero (p=??), but not significantly different from each other (p=??).  This suggests that we may be seeing a thresholding effect above one second sensor dropouts.
% TODO: table with values

To investigate this threshold, we had seven participants complete an extra three trials with the medium field of view and sensor dropout periods of 0.5 seconds.  Figure \ref{fig:extra} shows a box plot of time tracking versus sensor dropout length for these seven participants.  Again using Tukey's HSD to compare the means, we did not find a significant difference between the zero length and half second length conditions, but did find a significant difference between those and the higher length conditions.  
%TODO: table with values
With further experimentation, we could further delineate the performance curve as the dropout length increases from zero to one.
%For future work, we could find the threshold between 0.5 and 1.

\begin{figure}[htb]
	\centering
	\includegraphics[width=3.5in]{figures/extra.pdf}
	\caption{Box plot of the number of times the participant loses the target versus the length of the sensor dropout periods, including the half second dropout condition.  We collected this data from seven participants, using the medium field of view.}
\end{figure}

%use multiple comparison of means to determine which pairs are significant -- difference between length of one and length of two is not significant.

\section{Conclusions and Future Work}



%\section*{Acknowledgements}
%Cha Lee, Steffen Gauglitz, funding, and all of our participants.

\bibliographystyle{acmsiggraph}
\nocite{*}
\bibliography{paper}
\end{document}
